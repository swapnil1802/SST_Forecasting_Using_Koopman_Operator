{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"Dataset/\"\n",
    "\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "# Calculate global minimum and maximum SST values across all years\n",
    "global_min_sst = float('inf')\n",
    "global_max_sst = float('-inf')\n",
    "\n",
    "for year in range(1981, 2024 + 1):\n",
    "    year_str = str(year)\n",
    "    file_path = os.path.join(data_dir, f'sst.day.mean.{year}.nc')  \n",
    "\n",
    "    try:\n",
    "        \n",
    "        dataset = xr.open_dataset(file_path)\n",
    "\n",
    "        # Subset the dataset for the smaller region of interest\n",
    "        region_sst = dataset['sst'].where(\n",
    "            (dataset['lat'] >= -2) & (dataset['lat'] <= 2) &  \n",
    "            (dataset['lon'] >= 234) & (dataset['lon'] <= 240),  \n",
    "            drop=True  # Drop points outside this range\n",
    "        )\n",
    "\n",
    "        # Update global min and max\n",
    "        year_min = region_sst.min().values\n",
    "        year_max = region_sst.max().values\n",
    "        global_min_sst = min(global_min_sst, year_min)\n",
    "        global_max_sst = max(global_max_sst, year_max)\n",
    "\n",
    "        print(f\"Checked year {year} for global min and max.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File for {year} not found. Skipping this year.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {year}: {e}\")\n",
    "\n",
    "print(f\"Global Min SST: {global_min_sst}, Global Max SST: {global_max_sst}\")\n",
    "\n",
    "# Process and scale SST data for each year using global min and max\n",
    "for year in range(1982, 2024 + 1):\n",
    "    year_str = str(year)\n",
    "    file_path = os.path.join(data_dir, f'sst.day.mean.{year}.nc')\n",
    "\n",
    "    try:\n",
    "        \n",
    "        dataset = xr.open_dataset(file_path)\n",
    "\n",
    "        # Subset the dataset for the smaller region of interest\n",
    "        region_sst = dataset['sst'].where(\n",
    "            (dataset['lat'] >= -2) & (dataset['lat'] <= 2) &  \n",
    "            (dataset['lon'] >= 234) & (dataset['lon'] <= 240),  \n",
    "            drop=True  # Drop points outside this range\n",
    "        )\n",
    "        # Check if there are any NaN values in the region_sst\n",
    "        has_nan = region_sst.isnull().any()\n",
    "\n",
    "        \n",
    "        if has_nan:\n",
    "            print(\"The region_sst contains NaN values.\")\n",
    "        else:\n",
    "            print(\"The region_sst does not contain any NaN values.\")\n",
    "        # Scale SST values using global min and max\n",
    "        region_sst_scaled = (2 * (region_sst - global_min_sst) / (global_max_sst - global_min_sst)) - 1\n",
    "\n",
    "        \n",
    "        processed_data[year_str] = region_sst_scaled\n",
    "        print(f\"Processed year {year}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File for {year} not found. Skipping this year.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {year}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_dim + hidden_dim, 4 * hidden_dim, kernel_size,\n",
    "            padding=kernel_size[0] // 2, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, kernel_size)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        batch_size, seq_length, _, height, width = input_tensor.size()\n",
    "        hidden_states = [\n",
    "            (torch.zeros(batch_size, layer.hidden_dim, height, width, device=input_tensor.device),\n",
    "             torch.zeros(batch_size, layer.hidden_dim, height, width, device=input_tensor.device))\n",
    "            for layer in self.layers\n",
    "        ]\n",
    "        for t in range(seq_length):\n",
    "            x = input_tensor[:, t]\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                h, c = hidden_states[i]\n",
    "                h, c = layer(x, (h, c))\n",
    "                hidden_states[i] = (h, c)\n",
    "                x = h\n",
    "        return x\n",
    "\n",
    "class ConvLSTMWithOutput(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, n_days):\n",
    "        super(ConvLSTMWithOutput, self).__init__()\n",
    "        self.convlstm = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers)\n",
    "        self.conv_output = nn.Conv2d(hidden_dim, n_days, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        lstm_output = self.convlstm(input_tensor)\n",
    "        output = self.conv_output(lstm_output)\n",
    "        return output  # Shape: [batch_size, n_days, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, n_days):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.n_days = n_days\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - (self.seq_length + self.n_days - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx:idx + self.seq_length]\n",
    "        target_seq = self.data[idx + self.seq_length:idx + self.seq_length + self.n_days]\n",
    "        input_seq = input_seq[np.newaxis, :, :, :]  # Add channel dim\n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seq_length = 30\n",
    "n_days = 30\n",
    "batch_size = 24\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data Preparation\n",
    "train_years = list(range(1982, 2006))\n",
    "val_years = list(range(2006, 2015))\n",
    "train_data = np.concatenate([processed_data[str(year)] for year in train_years], axis=0)\n",
    "val_data = np.concatenate([processed_data[str(year)] for year in val_years], axis=0)\n",
    "\n",
    "train_dataset = SSTDataset(train_data, seq_length, n_days)\n",
    "val_dataset = SSTDataset(val_data, seq_length, n_days)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvLSTMWithOutput(input_dim=30, hidden_dim=64, kernel_size=(3, 3), num_layers=5, n_days=n_days).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seq_length = 30\n",
    "n_days = 30\n",
    "batch_size = 24\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data Preparation\n",
    "train_years = list(range(1982, 2006))\n",
    "val_years = list(range(2006, 2015))\n",
    "train_data = np.concatenate([processed_data[str(year)] for year in train_years], axis=0)\n",
    "val_data = np.concatenate([processed_data[str(year)] for year in val_years], axis=0)\n",
    "\n",
    "train_dataset = SSTDataset(train_data, seq_length, n_days)\n",
    "val_dataset = SSTDataset(val_data, seq_length, n_days)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvLSTMWithOutput(input_dim=30, hidden_dim=64, kernel_size=(3, 3), num_layers=5, n_days=n_days).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"convlstm_30D_nino.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute MAE, RMSE, and R² between predictions and targets.\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted SST values, shape\n",
    "        targets: Ground truth SST values, shape\n",
    "\n",
    "    Returns:\n",
    "        mae: Mean Absolute Error.\n",
    "        rmse: Root Mean Square Error.\n",
    "        r2: Coefficient of Determination (R²).\n",
    "    \"\"\"\n",
    "    mae = np.mean(np.abs(predictions - targets))  # Mean Absolute Error\n",
    "    mse = np.mean((predictions - targets) ** 2)  # Mean Squared Error\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "\n",
    "    # Compute R²\n",
    "    ss_total = np.sum((targets - targets.mean()) ** 2)  # Total sum of squares\n",
    "    ss_residual = np.sum((targets - predictions) ** 2)  # Residual sum of squares\n",
    "    r2 = 1 - (ss_residual / ss_total) if ss_total > 0 else 0\n",
    "\n",
    "    return mae, rmse, r2\n",
    "\n",
    "\n",
    "def inverse_scale(data, global_min_sst, global_max_sst):\n",
    "    return ((data + 1) / 2) * (global_max_sst - global_min_sst) + global_min_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Function\n",
    "def test_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_predictions, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    return np.concatenate(all_predictions), np.concatenate(all_targets)\n",
    "\n",
    "# Test Data\n",
    "test_years = list(range(2015, 2025))  \n",
    "test_data = np.concatenate([processed_data[str(year)] for year in test_years], axis=0)\n",
    "test_dataset = SSTDataset(test_data, seq_length, n_days)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load(\"convlstm_30D_nino.pth\"))\n",
    "test_predictions, test_targets = test_model(model, test_loader, device)\n",
    "\n",
    "# Rescale and Evaluate\n",
    "test_predictions_rescaled = inverse_scale(test_predictions, global_min_sst, global_max_sst)\n",
    "test_targets_rescaled = inverse_scale(test_targets, global_min_sst, global_max_sst)\n",
    "mae, rmse, r2 = compute_metrics(test_predictions_rescaled, test_targets_rescaled)\n",
    "\n",
    "print(f\"Test MAE: {mae:.4f} °C\")\n",
    "print(f\"Test RMSE: {rmse:.4f} °C\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to plot heatmap for a specific day\n",
    "def plot_heatmap(data, title, day_index, global_min_sst, global_max_sst):\n",
    "    \"\"\"\n",
    "    Plot a heatmap for a specific day.\n",
    "    \n",
    "    Args:\n",
    "        data: The SST data (shape: [steps, days, height, width]).\n",
    "        title: Title for the plot.\n",
    "        day_index: Index for the day to be plotted (aligned with test_targets).\n",
    "        global_min_sst: Minimum SST value for consistent color scale.\n",
    "        global_max_sst: Maximum SST value for consistent color scale.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(data, cmap='coolwarm', vmin=global_min_sst, vmax=global_max_sst)\n",
    "    plt.colorbar(label=\"Sea Surface Temperature (°C)\")\n",
    "    plt.title(f\"{title} (Day {day_index + 30})\", fontsize=14)\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "day_to_plot = 31  # Adjust this value as needed\n",
    "\n",
    "# Map the actual day in the test dataset to the index in test_targets\n",
    "# The first valid day in test_targets corresponds to day seq_length + n_days - 1\n",
    "seq_length = 30\n",
    "n_days = 30\n",
    "aligned_day_index = day_to_plot - seq_length\n",
    "\n",
    "# Ensure the day is valid\n",
    "if aligned_day_index < 0 or aligned_day_index >= test_targets.shape[0]:\n",
    "    print(f\"Invalid day_to_plot: {day_to_plot}. It must be between {seq_length} and {test_targets.shape[0] + seq_length - 1}.\")\n",
    "else:\n",
    "    \n",
    "    # Get the heatmap data for the specific day\n",
    "    ground_truth_heatmap = test_targets_rescaled[aligned_day_index, -1]  # Last predicted day in target\n",
    "    prediction_heatmap = test_predictions_rescaled[aligned_day_index, -1]  # Last predicted day in prediction\n",
    "\n",
    "    # Plot the ground truth and prediction heatmaps\n",
    "    plot_heatmap(ground_truth_heatmap, f\"Ground Truth SST\", day_to_plot, global_min_sst, global_max_sst)\n",
    "    plot_heatmap(prediction_heatmap, f\"Prediction SST (ConvLSTM)\", day_to_plot, global_min_sst, global_max_sst)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
